### Screencast Summary



| Screencast | Date | Notable Topics | Annotated | Link | Data |
| :--- | --- | --- | :---: | :---: | :---: |
| College Majors and Income | 2018-10-15 |  | :x: | [:link:](https://www.youtube.com/watch?v=nx5yhXAQLxw) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-10-16) |
| Horror Movie Profits | 2018-10-23 |  | :x: | [:link:](https://www.youtube.com/watch?v=3-DRwg9yeNA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-10-23) |
| R Downloads | 2018-10-30 |  | :x: | [:link:](https://www.youtube.com/watch?v=nms9F-XubJU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/blob/master/data/2018/2018-11-06) |
| [US Wind Turbines](#us-wind-turbines) | 2018-11-06 | Animated map using `gganimate` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=O1oDIQV6VKU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-06) |
| Malaria Incidence | 2018-11-12 |  | :x: | [:link:](https://www.youtube.com/watch?v=5_6O2oDy5Jk) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-13) |
| [Thanksgiving Dinner](#thanksgiving-dinner) | 2018-11-21 | Survey data, Network graphing | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=rxJZT0duwfU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-20) |
| [Maryland Bridges](#maryland-bridges) | 2018-11-27 | Data manipulation, Map visualization | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=kzM-4jMh9Qs) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-27) |
| [Medium Articles](#medium-articles) | 2018-12-04 | Text mining using `tidytext` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=C69QyycHsgE) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-04) |
| [Riddler: Monte Carlo Simulation](#riddler:-monte-carlo-simulation) | 2018-12-04 | Simulation | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=pBGMt28xgvk) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/the-riddler-just-had-to-go-and-reinvent-beer-pong/) |
| [NYC Restaurant Inspections](#nyc-restaurant-inspections) | 2018-12-11 | Multiple t-test models using `broom`, Principal Component Analysis (PCA) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=em4FXPf4H-Y) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-11) |
| Riddler: Simulating a Week of Rain | 2018-12-12 |  | :x: | [:link:](https://www.youtube.com/watch?v=TDzd73z8thU) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/the-little-mathematically-determined-house-on-the-prairie/) |
| [Dolphins](#dolphins) | 2018-12-18 | Survival analysis using `survival` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=KiqpX-gNIS4) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-18) |
| [TidyTuesday Tweets](#tidytuesday-tweets) | 2019-01-07 | Text mining using `tidytext` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=KE9ItC3doEU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01) |
| TV Golden Age | 2019-01-09 |  | :x: | [:link:](https://www.youtube.com/watch?v=oYGi2wgSJaM) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-08) |
| Space Launches | 2019-01-15 |  | :x: | [:link:](https://www.youtube.com/watch?v=ZyPrP_Yo1BA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-15) |
| US Incarceration | 2019-01-25 |  | :x: | [:link:](https://www.youtube.com/watch?v=78kv808ZU6o) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-22) |
| [US Dairy Consumption](#us-dairy-consumption) | 2019-01-29 | Time series analysis, Forecasting using `sweep` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=13iG_HkEPVc) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-29) |
| [US PhDs](#us-phds) | 2019-02-22 | Tidying very un-tidy data | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=KzRP40PzopY) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-19) |
| [French Train Delays](#french-train-delays) | 2019-02-26 | Heat map | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=bmaigtpKyiM) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-26) |
| Women in the Workplace | 2019-03-05 |  | :x: | [:link:](https://www.youtube.com/watch?v=fv9SQ4IFNr4) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-05) |
| Board Game Reviews | 2019-03-15 |  | :x: | [:link:](https://www.youtube.com/watch?v=qirKGdQvy9U) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-12) |
| Seattle Pet Names | 2019-03-16 |  | :x: | [:link:](https://www.youtube.com/watch?v=EF4A4OtQprg) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-26) |
| [Seattle Bike Counts](#seattle-bike-counts) | 2019-04-05 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=sBho2GJE5lc) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-02) |
| [Tennis Tournaments](#tennis-tournaments) | 2019-04-09 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=YWUCUfEeNJI) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-09) |
| [Bird Collisions](#bird-collisions) | 2019-05-03 | Bootstrapping | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=zjWm__nFLXI) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-30) |
| [Student-Teacher Ratios](#studentteacher-ratios) | 2019-05-10 | `WDI` package (World Development Indicators) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=NoUHdrailxA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-07) |
| Nobel Prize Winners | 2019-05-24 |  | :x: | [:link:](https://www.youtube.com/watch?v=yWSpLfmES7w) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-14) |
| [Plastic Waste](#plastic-waste) | 2019-05-27 | Choropleth map | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=BRdLOYtJk9o) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-21) |
| [Wine Ratings](#wine-ratings) | 2019-05-31 | Text mining using `tidytext`, Lasso regression using `glmnet` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=AQzZNIyjyWM) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-28) |
| [Ramen Reviews](#ramen-reviews) | 2019-06-04 | Web scraping using `rvest` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=tCa2di7aEP4) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-06-04) |
| [Media Franchise Revenue](#media-franchise-revenue) | 2019-06-22 | Data manipulation (especially re-ordering factors) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=1xsbTs9-a50) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-02) |
| [Women's World Cup](#womens-world-cup) | 2019-07-22 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=ZOQSuapvHqA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-09) |
| [Bob Ross Paintings](#bob-ross-paintings) | 2019-08-12 | Network graphs, Principal Component Analysis (PCA) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=sD993H5FBIY) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-08-06) |
| Simpsons Guest Stars | 2019-08-30 |  | :x: | [:link:](https://www.youtube.com/watch?v=EYuuAGDeGrQ) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-08-27) |
| [Pizza Ratings](#pizza-ratings) | 2019-10-01 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=Mkac8DHScps) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-01) |
| [Car Fuel Efficiency](#car-fuel-efficiency) | 2019-10-15 | Natural splines for regression | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=RpeioixHOHw) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-15) |
| [Horror Movies](#horror-movies) | 2019-10-22 | ANOVA, Text mining using `tidytext`, Lasso regression using `glmnet` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=yFRSTlk3kRQ) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-22) |
| [NYC Squirrel Census](#nyc-squirrel-census) | 2019-11-01 | Map visualization using `ggmap` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=6GV9sAD6Pi0) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-29) |
| [CRAN Package Code](#cran-package-code) | 2019-12-30 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=dr4qw8o0nYU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-11-12) |
| Riddler: Spelling Bee Honeycomb | 2020-01-06 |  | :x: | [:link:](https://www.youtube.com/watch?v=wFZhuQEfEYA) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/can-you-solve-the-vexing-vexillology/) |



***



### Individual Screencasts



#### US Wind Turbines

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US Wind Turbines | [3:50](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=230s) | Using `count` function to explore categorical variables |
| US Wind Turbines | [5:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=300s) | Creating a quick-and-dirty map using `geom_point` function and latitude and longitude data |
| US Wind Turbines | [6:10](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=370s) | Explaining need for `mapproj` package when plotting maps in `ggplot2` |
| US Wind Turbines | [7:35](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=455s) | Using `borders` function to add US state borders to map |
| US Wind Turbines | [10:45](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=645s) | Using `fct_lump` function to get the top 6 project categories and put the rest in a lumped "Other" category |
| US Wind Turbines | [11:30](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=690s) | Changing data so that certain categories' points appear in front of other categories' points on the map |
| US Wind Turbines | [14:15](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=855s) | Taking the centroid (average longitude and latitude) of points across a geographic area as a way to aggregate categories to one point |
| US Wind Turbines | [19:40](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=1180s) | Using `ifelse` function to clean missing data that is coded as "-9999" |
| US Wind Turbines | [26:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=1560s) | Asking, "How has turbine capacity changed over time?" |
| US Wind Turbines | [33:15](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=1995s) | Exploring different models of wind turbines |
| US Wind Turbines | [38:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2280s) | Using `mutate_if` function to find NA values (coded as -9999) in multiple columns and replace them with an actual NA |
| US Wind Turbines | [45:40](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2740s) | Reviewing documentation for `gganimate` package |
| US Wind Turbines | [47:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2820s) | Attempting to set up `gganimate` map |
| US Wind Turbines | [48:55](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2935s) | Understanding `gganimate` package using a "Hello World" / toy example, then trying to debug turbine animation |
| US Wind Turbines | [56:45](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3405s) | Using `is.infinite` function to get rid of troublesome Inf values |
| US Wind Turbines | [57:55](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3475s) | Quick hack for getting cumulative data from a table using `crossing` function (though it does end up with some duplication) |
| US Wind Turbines | [1:01:45](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3705s) | Diagnosis of `gganimate` issue (points between integer years are being interpolated) |
| US Wind Turbines | [1:04:35](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3875s) | Pseudo-successful `gganimate` map (cumulative points show up, but some points are missing) |
| US Wind Turbines | [1:05:40](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3940s) | Summary of screencast |



***



#### Thanksgiving Dinner

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Thanksgiving Dinner | [4:10](https://www.youtube.com/watch?v=rxJZT0duwfU&t=250s) | Exploratory bar chart of age distribution (and gender) of survey respondents |
| Thanksgiving Dinner | [7:40](https://www.youtube.com/watch?v=rxJZT0duwfU&t=460s) | Using `count` function on multiple columns to get detailed counts |
| Thanksgiving Dinner | [11:25](https://www.youtube.com/watch?v=rxJZT0duwfU&t=685s) | Parsing numbers from text using `parse_number` function, then using those numbers to re-level an ordinal factor (income bands) |
| Thanksgiving Dinner | [13:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=785s) | Exploring relationship between income and using homemade (vs. canned) cranberry sauce |
| Thanksgiving Dinner | [14:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=840s) | Adding group = 1 argument to the `aes` function to properly display a line chart |
| Thanksgiving Dinner | [14:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=870s) | Rotating text for axis labels that overlap |
| Thanksgiving Dinner | [16:50](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1010s) | Getting confidence intervals for proportions using Jeffreys interval (using beta distribution with an uniformative prior) |
| Thanksgiving Dinner | [17:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1075s) | Explanation of Clopper-Pearson approach as alternative to Jeffreys interval |
| Thanksgiving Dinner | [18:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1110s) | Using `geom_ribbon` function add shaded region to line chart that shows confidence intervals |
| Thanksgiving Dinner | [21:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1315s) | Using `starts_with` function to select fields with names that start with a certain string (e.g., using "pie" selects "pie1" and "pie2") |
| Thanksgiving Dinner | [22:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1375s) | Using `gather` function to get wide-format data to tidy (tall) format |
| Thanksgiving Dinner | [23:45](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1425s) | Using `str_remove` and regex to remove digits from field values (e.g., "dessert1" and "dessert2" get turned into "dessert") |
| Thanksgiving Dinner | [27:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1620s) | "What are people eating?" Graphing pies, sides, and desserts |
| Thanksgiving Dinner | [28:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1680s) | Using `fct_reorder` function to reorder foods based on how popular they are |
| Thanksgiving Dinner | [28:45](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1725s) | Using `n_distinct` function count the number of unique respondents |
| Thanksgiving Dinner | [30:25](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1825s) | Using `facet_wrap` function to facet food types into their own graphs |
| Thanksgiving Dinner | [32:50](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1970s) | Using `parse_number` function to convert age ranges as character string into a numeric field |
| Thanksgiving Dinner | [35:35](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2135s) | Exploring relationship between US region and food types |
| Thanksgiving Dinner | [36:15](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2175s) | Using `group_by`, then `mutate`, then `count` to calculate a complicated summary |
| Thanksgiving Dinner | [40:35](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2435s) | Exploring relationship between praying at Thanksgiving (yes/no) and food types |
| Thanksgiving Dinner | [42:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2550s) | Empirical Bayes binomial estimation for calculating binomial confidence intervals (see [Dave's book on Empirical Bayes](https://gumroad.com/l/empirical-bayes)) |
| Thanksgiving Dinner | [45:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2730s) | Asking, "What sides/desserts/pies are eaten together?" |
| Thanksgiving Dinner | [46:20](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2780s) | Calculating pairwise correlation of food types |
| Thanksgiving Dinner | [49:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2945s) | Network graph of pairwise correlation |
| Thanksgiving Dinner | [51:40](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3100s) | Adding text labels to nodes using `geom_node_text` function |
| Thanksgiving Dinner | [53:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3180s) | Getting rid of unnecessary graph elements (e.g., axes, gridlines) with `theme_void` function |
| Thanksgiving Dinner | [53:25](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3205s) | Explanation of network graph relationships |
| Thanksgiving Dinner | [55:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3305s) | Adding dimension to network graph (node colour) to represent the type of food |
| Thanksgiving Dinner | [57:45](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3465s) | Fixing overlapping text labels using the `geom_node_text` function's repel argument |
| Thanksgiving Dinner | [58:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3535s) | Tweaking display of percentage legend to be in more readable format (e.g., "40%" instead of "0.4") |
| Thanksgiving Dinner | [1:00:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3605s) | Summary of screencast |



***



#### Maryland Bridges

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Maryland Bridges | [9:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=555s) | Using `geom_line` to create an exploratory line graph |
| Maryland Bridges | [10:10](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=610s) | Using `%/%` operator (truncated division) to bin years into decades (e.g., 1980, 1984, and 1987 would all become "1980") |
| Maryland Bridges | [12:30](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=750s) | Converting two-digit year to four-digit year (e.g., "16" becomes "2016") by adding 2000 to each one |
| Maryland Bridges | [15:40](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=940s) | Using `percent_format` function from `scales` package to get nice-looking axis labels |
| Maryland Bridges | [19:55](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1195s) | Using `geom_col` to create an ordered nice bar/column graph |
| Maryland Bridges | [21:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1295s) | Using `replace_na` to replace NA values with "Other" |
| Maryland Bridges | [27:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1635s) | Starting exploration of average daily traffic |
| Maryland Bridges | [29:05](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1745s) | Using `comma_format` function from `scales` package to get more readable axis labels (e.g., "1e+05" becomes "100,000") |
| Maryland Bridges | [31:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1875s) | Using `cut` function to bin continuous variable into customized breaks (also does a `mutate` within a `group_by`!) |
| Maryland Bridges | [34:30](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2070s) | Starting to make a map |
| Maryland Bridges | [37:00](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2220s) | Encoding a continuous variable to colour, then using `scale_colour_gradient2` function to specify colours and midpoint |
| Maryland Bridges | [38:20](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2300s) | Specifying the `trans` argument (transformation) of the `scale_colour_gradient2` function to get a logarithmic scale |
| Maryland Bridges | [45:55](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2755s) | Using `str_to_title` function to get values to Title Case (first letter of each word capitalized) |
| Maryland Bridges | [48:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2915s) | Predicting whether bridges are in "Good" condition using logistic regression (remember to specify the family argument! Dave fixes this at 52:54) |
| Maryland Bridges | [50:30](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3030s) | Explanation of why we should NOT be using an OLS linear regression |
| Maryland Bridges | [51:10](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3070s) | Using the `augment` function from the `broom` package to illustrate why a linear model is not a good fit |
| Maryland Bridges | [52:05](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3125s) | Specifying the `type.predict` argument in the `augment` function so that we get the actual predicted probability |
| Maryland Bridges | [54:40](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3280s) | Explanation of why the sigmoidal shape of logistic regression can be a drawback |
| Maryland Bridges | [55:05](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3305s) | Using a cubic spline model (a type of GAM, Generalized Additive Model) as an alternative to logistic regression |
| Maryland Bridges | [56:00](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3360s) | Explanation of the shape that a cubic spline model can take (which logistic regression cannot) |
| Maryland Bridges | [1:02:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3735s) | Visualizing the model in a different way, using a coefficient plot |
| Maryland Bridges | [1:04:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3875s) | Using `geom_vline` function to add a red reference line to a graph |
| Maryland Bridges | [1:04:50](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3890s) | Adding confidence intervals to the coefficient plot by specifying `conf.int` argument of `tidy` function and graphing using the `geom_errorbarh` function |
| Maryland Bridges | [1:05:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3935s) | Brief explanation of log-odds coefficients |
| Maryland Bridges | [1:09:10](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=4150s) | Summary of screencast |



***



#### Medium Articles

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Medium Articles | [5:40](https://www.youtube.com/watch?v=C69QyycHsgE&t=340s) | Using `summarise_at` and `starts_with` functions to quickly sum up all variables starting with "tag_" |
| Medium Articles | [6:55](https://www.youtube.com/watch?v=C69QyycHsgE&t=415s) | Using `gather` function (now `pivot_longer`) to convert topic tag variables from wide to tall (tidy) format |
| Medium Articles | [8:10](https://www.youtube.com/watch?v=C69QyycHsgE&t=490s) | Explanation of how gathering step above will let us find the most/least common tags |
| Medium Articles | [9:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=540s) | Explanation of using `median` (instead of `mean`) as measure of central tendency for number of claps an article got |
| Medium Articles | [9:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=590s) | Visualizing log-normal (ish) distribution of number of claps an article gets |
| Medium Articles | [12:05](https://www.youtube.com/watch?v=C69QyycHsgE&t=725s) | Using `pmin` function to bin reading times of 10 minutes or more to cap out at 10 minutes |
| Medium Articles | [12:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=755s) | Changing `scale_x_continuous` function's `breaks` argument to get custom labels and tick marks on a histogram |
| Medium Articles | [14:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=875s) | Discussion of using mean vs. median as measure of central tendency for reading time (he decides on mean) |
| Medium Articles | [16:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=960s) | Starting text mining analysis |
| Medium Articles | [16:40](https://www.youtube.com/watch?v=C69QyycHsgE&t=1000s) | Using `unnest_tokens` function from `tidytext` package to split character string into individual words |
| Medium Articles | [17:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=1070s) | Explanation of stop words and using `anti_join` function to get rid of them |
| Medium Articles | [20:20](https://www.youtube.com/watch?v=C69QyycHsgE&t=1220s) | Using `str_detect` function to filter out "words" that are just numbers (e.g., "2", "35") |
| Medium Articles | [22:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=1355s) | Quick analysis of which individual words are associated with more/fewer claps ("What are the hype words?") |
| Medium Articles | [25:15](https://www.youtube.com/watch?v=C69QyycHsgE&t=1515s) | Using geometric mean as alternative to median to get more distinction between words (note 27:33 where he makes a quick fix) |
| Medium Articles | [28:10](https://www.youtube.com/watch?v=C69QyycHsgE&t=1690s) | Starting analysis of clusters of related words (e.g., "neural" is linked to "network") |
| Medium Articles | [30:30](https://www.youtube.com/watch?v=C69QyycHsgE&t=1830s) | Finding correlations pairs of words using `pairwise_cor` function from `widyr` package |
| Medium Articles | [34:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=2040s) | Using `ggraph` and `igraph` packages to make network plot of correlated pairs of words |
| Medium Articles | [35:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=2100s) | Using `geom_node_text` to add labels for points (vertices) in the network plot |
| Medium Articles | [38:40](https://www.youtube.com/watch?v=C69QyycHsgE&t=2320s) | Filtering original data to only include words appear in the network plot (150 word pairs with most correlation) |
| Medium Articles | [40:10](https://www.youtube.com/watch?v=C69QyycHsgE&t=2410s) | Adding colour as a dimension to the network plot, representing geometric mean of claps |
| Medium Articles | [40:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=2450s) | Changing default colour scale to one with Blue = Low and High = Red with `scale_colour_gradient2` function |
| Medium Articles | [43:15](https://www.youtube.com/watch?v=C69QyycHsgE&t=2595s) | Adding dark outlines to points on network plot with a hack |
| Medium Articles | [44:45](https://www.youtube.com/watch?v=C69QyycHsgE&t=2685s) | Starting to predict number of claps based on title tag (Lasso regression) |
| Medium Articles | [45:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=2750s) | Explanation of data format needed to conduct Lasso regression (and using `cast_sparse` function to get sparse matrix) |
| Medium Articles | [47:45](https://www.youtube.com/watch?v=C69QyycHsgE&t=2865s) | Bringing in number of claps to the sparse matrix (un-tidy methods) |
| Medium Articles | [49:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=2940s) | Using `cv.glmnet` function (cv = cross validated) from `glmnet` package to run Lasso regression |
| Medium Articles | [49:55](https://www.youtube.com/watch?v=C69QyycHsgE&t=2995s) | Finding and fixing mistake in defining Lasso model |
| Medium Articles | [51:05](https://www.youtube.com/watch?v=C69QyycHsgE&t=3065s) | Explanation of Lasso model |
| Medium Articles | [52:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=3155s) | Using `tidy` function from the `broom` package to tidy up the Lasso model |
| Medium Articles | [54:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=3275s) | Visualizing how specific words affect the prediction of claps as lambda (Lasso's penalty parameter) changes |
| Medium Articles | [1:00:20](https://www.youtube.com/watch?v=C69QyycHsgE&t=3620s) | Summary of screencast |



***

#### Riddler: Monte Carlo Simulation

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Riddler: Monte Carlo Simulation | [3:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=190s) | Using `crossing` function to set up structure of simulation (1,000 trials, each with 12 chess games) |
| Riddler: Monte Carlo Simulation | [4:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=240s) | Adding result to the tidy simulation dataset |
| Riddler: Monte Carlo Simulation | [6:45](https://www.youtube.com/watch?v=pBGMt28xgvk&t=405s) | Using `sample` function to simulate win/loss/draw for each game (good explanation of individual arguments within sample) |
| Riddler: Monte Carlo Simulation | [7:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=425s) | Using `group_by` and `summarise` to get total points for each trial |
| Riddler: Monte Carlo Simulation | [8:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=490s) | Adding red vertical reference line to histogram to know when a player wins a matchup |
| Riddler: Monte Carlo Simulation | [10:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=600s) | Answering second piece of riddle (how many games would need to be played for better player to win 90% or 99% of the time?) |
| Riddler: Monte Carlo Simulation | [10:50](https://www.youtube.com/watch?v=pBGMt28xgvk&t=650s) | Using `unnest` and `seq_len` functions to create groups of number of games (20, 40, …, 100), each with one game per row |
| Riddler: Monte Carlo Simulation | [12:15](https://www.youtube.com/watch?v=pBGMt28xgvk&t=735s) | Creating a win field based on the simulated data, then summarising win percentage for each group of number of games (20, 40, …, 100) |
| Riddler: Monte Carlo Simulation | [13:55](https://www.youtube.com/watch?v=pBGMt28xgvk&t=835s) | Using `seq` function to create groups of number of games programmatically |
| Riddler: Monte Carlo Simulation | [15:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=905s) | Explanation of using logarithmic scale for this riddle |
| Riddler: Monte Carlo Simulation | [15:45](https://www.youtube.com/watch?v=pBGMt28xgvk&t=945s) | Changing spacing of number of games from even spacing (20, 40, …, 100) to exponential (doubles every time, 12, 24, 48, …, 1536) |
| Riddler: Monte Carlo Simulation | [18:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1080s) | Changing spacing of number of games to be finer |
| Riddler: Monte Carlo Simulation | [19:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1140s) | Introduction of interpolation as the last step we will do |
| Riddler: Monte Carlo Simulation | [19:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1170s) | Introducing `approx` function as method to linearly interpolate data |
| Riddler: Monte Carlo Simulation | [22:35](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1355s) | Break point for the next riddle |
| Riddler: Monte Carlo Simulation | [24:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1470s) | Starting recursive approach to this riddle |
| Riddler: Monte Carlo Simulation | [25:35](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1535s) | Setting up a N x N matrix (N = 4 to start) |
| Riddler: Monte Carlo Simulation | [25:55](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1555s) | Explanation of approach (random ball goes into random cup, represented by matrix) |
| Riddler: Monte Carlo Simulation | [26:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1585s) | Using `sample` function to pick a random element of the matrix |
| Riddler: Monte Carlo Simulation | [27:15](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1635s) | Using for loop to iterate random selection 100 times |
| Riddler: Monte Carlo Simulation | [28:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1705s) | Converting for loop to while loop, using `colSums` to keep track of number of balls in cups |
| Riddler: Monte Carlo Simulation | [30:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1805s) | Starting to code the pruning phase |
| Riddler: Monte Carlo Simulation | [30:15](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1815s) | Using `diag` function to pick matching matrix elements (e.g., the 4th row of the 4th column) |
| Riddler: Monte Carlo Simulation | [31:50](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1910s) | Turning code up to this point into a custom simulate_round function |
| Riddler: Monte Carlo Simulation | [32:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1945s) | Using custom simulate_round function to simulate 100 rounds |
| Riddler: Monte Carlo Simulation | [33:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2010s) | Using `all` function to perform logic check on whether all cups in a round are not empty |
| Riddler: Monte Carlo Simulation | [34:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2045s) | Converting loop approach to tidy approach |
| Riddler: Monte Carlo Simulation | [35:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2110s) | Using `rerun` and `map_lgl` functions from `purrr` to simulate a round for each for in a dataframe |
| Riddler: Monte Carlo Simulation | [36:20](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2180s) | Explanation of the tidy approach |
| Riddler: Monte Carlo Simulation | [37:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2225s) | Using `cumsum` and `lag` functions to keep track of the number of rounds until you win a "game" |
| Riddler: Monte Carlo Simulation | [39:45](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2385s) | Creating histogram of number of rounds until winning a game |
| Riddler: Monte Carlo Simulation | [40:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2410s) | Setting boundary argument of `geom_histogram` function to include count of zeros |
| Riddler: Monte Carlo Simulation | [40:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2430s) | Brief explanation of geometric distribution |
| Riddler: Monte Carlo Simulation | [41:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2485s) | Extending custom simulate_round function to include number of balls thrown to win (in addition to whether we won a round) |
| Riddler: Monte Carlo Simulation | [46:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2770s) | Extending to two values of N (N = 3 or N = 4) |
| Riddler: Monte Carlo Simulation | [49:50](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2990s) | Reviewing results of N = 3 and N = 4 |
| Riddler: Monte Carlo Simulation | [52:20](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3140s) | Extending to N = 5 |
| Riddler: Monte Carlo Simulation | [53:55](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3235s) | Checking results of chess riddle with Riddler solution |
| Riddler: Monte Carlo Simulation | [55:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3310s) | Checking results of ball-cup riddle with Riddler solution (Dave slightly misinterpreted what the riddle was asking) |
| Riddler: Monte Carlo Simulation | [56:35](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3395s) | Changing simulation code to correct the misinterpretation |
| Riddler: Monte Carlo Simulation | [1:01:40](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3700s) | Reviewing results of corrected simulation |
| Riddler: Monte Carlo Simulation | [1:03:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3810s) | Checking results of ball-cup riddle with corrected simulation with Riddler solutions |
| Riddler: Monte Carlo Simulation | [1:06:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3960s) | Visualizing number of balls thrown and rounds played |



***



#### NYC Restaurant Inspections

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| NYC Restaurant Inspections | [18:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1125s) | Separating column using `separate` function |
| NYC Restaurant Inspections | [21:15](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1275s) | Taking distinct observations, but keeping the remaining variables using `distinct` function with .keep_all argument |
| NYC Restaurant Inspections | [25:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1500s) | Using `broom` package and `nest` function to perform multiple t-tests at the same time |
| NYC Restaurant Inspections | [26:20](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1580s) | Tidying nested t-test models using `broom` |
| NYC Restaurant Inspections | [27:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1620s) | Creating TIE fighter plot of estimates of means and their confidence intervals |
| NYC Restaurant Inspections | [28:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1725s) | Recode long description using regex to remove everything after a parenthesis |
| NYC Restaurant Inspections | [33:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=2025s) | Using `cut` function to manually bin data along user-specified intervals |
| NYC Restaurant Inspections | [42:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=2520s) | Asking, "What type of violations tend to occur more in some cuisines than others?" |
| NYC Restaurant Inspections | [42:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=2565s) | Using `semi_join` function to get the most recent inspection of all the restaurants |
| NYC Restaurant Inspections | [52:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3120s) | Asking, "What violations tend to occur together?" |
| NYC Restaurant Inspections | [53:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3180s) | Using `widyr` package function `pairwise_cor` (pairwise correlation) to find co-occurrence of violation types |
| NYC Restaurant Inspections | [55:30](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3330s) | Beginning of PCA (Principal Component Analysis) using `widely_svd` function |
| NYC Restaurant Inspections | [58:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3480s) | Actually typing in the `widely_svd` function |
| NYC Restaurant Inspections | [58:15](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3495s) | Reviewing and explaining output of `widely_svd` function |
| NYC Restaurant Inspections | [1:01:30](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3690s) | Creating graph of opposing elements of a PCA dimension |
| NYC Restaurant Inspections | [1:02:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3720s) | Shortening string using `str_sub` function |
| NYC Restaurant Inspections | [1:04:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3840s) | Reference to [Julia Silge's PCA walkthrough](https://juliasilge.com/blog/stack-overflow-pca/) using StackOverflow data |



***



#### Dolphins

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Dolphins | [6:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=385s) | Using `year` function from `lubridate` package to simplify calculating age of dolphins |
| Dolphins | [8:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=510s) | Combining `count` and `fct_lump` functions to get counts of top 5 species (with other species lumped in "Other") |
| Dolphins | [9:55](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=595s) | Creating boxplot of species and age |
| Dolphins | [11:50](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=710s) | Dealing with different types of NA (double, logical) (he doesn't get it in this case, but it's still useful) |
| Dolphins | [15:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=930s) | Adding acquisition type as colour dimension to histogram |
| Dolphins | [16:00](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=960s) | Creating a spinogram of acquisition type over time (alternative to histogram) using `geom_area` |
| Dolphins | [17:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1045s) | Binning year into decade using truncated division operator `%/%` |
| Dolphins | [19:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1150s) | Fixing annoying triangular gaps in spinogram using complete function to fill in gaps in data |
| Dolphins | [21:15](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1275s) | Using `fct_reorder` function to reorder acquisition type (bigger categories are placed on the bottom of the spinogram) |
| Dolphins | [23:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1405s) | Adding vertical dashed reference line using `geom_vline` function |
| Dolphins | [24:05](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1445s) | Starting analysis of acquisition location |
| Dolphins | [27:05](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1625s) | Matching messy text data with regex to aggregate into a few categories variables with `fuzzyjoin` package |
| Dolphins | [31:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1890s) | Using `distinct` function's .keep_all argument to keep only one row per animal ID |
| Dolphins | [33:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1990s) | Using `coalesce` function to conditionally replace NAs (same functionality as SQL verb) |
| Dolphins | [40:00](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2400s) | Starting survival analysis |
| Dolphins | [46:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2785s) | Using `survfit` function from `survival` package to get a baseline survival curve (i.e., not regressed on any independent variables) |
| Dolphins | [47:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2850s) | Fixing cases where death year is before birth year |
| Dolphins | [48:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2910s) | Fixing specification of survfit model to better fit the format of our data (right-censored data) |
| Dolphins | [50:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3010s) | Built-in plot of baseline survival model (estimation of percentage survival at a given age) |
| Dolphins | [50:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3030s) | Using `broom` package to tidy the survival model data (which is better for `ggplot2` plotting) |
| Dolphins | [52:20](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3140s) | Fitting survival curve based on sex |
| Dolphins | [54:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3265s) | Cox proportional hazards model (to investigate association of survival time and one or more predictors) |
| Dolphins | [55:50](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3350s) | Explanation of why dolphins with unknown sex likely have a systematic bias with their data |
| Dolphins | [57:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3445s) | Investigating whether being born in captivity is associated with different survival rates |
| Dolphins | [1:00:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3610s) | Summary of screencast |



***



#### TidyTuesday Tweets

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| TidyTuesday Tweets | [1:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=80s) | Importing an rds file using `read_rds` function |
| TidyTuesday Tweets | [2:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=175s) | Using `floor_date` function from `lubridate` package to round dates down (that's what the floor part does) to the month level |
| TidyTuesday Tweets | [5:25](https://www.youtube.com/watch?v=KE9ItC3doEU&t=325s) | Asking, "Which tweets get the most re-tweets?" |
| TidyTuesday Tweets | [5:50](https://www.youtube.com/watch?v=KE9ItC3doEU&t=350s) | Using `contains` function to select only columns that contain a certain string ("retweet" in this case) |
| TidyTuesday Tweets | [8:05](https://www.youtube.com/watch?v=KE9ItC3doEU&t=485s) | Exploring likes/re-tweets ratio, including dealing with one or the other being 0 (which would cause divide by zero error) |
| TidyTuesday Tweets | [11:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=660s) | Starting exploration of actual text of tweets |
| TidyTuesday Tweets | [11:35](https://www.youtube.com/watch?v=KE9ItC3doEU&t=695s) | Using `unnest_tokens` function from `tidytext` package to break tweets into individual words (using token argument specifically for tweet-style text) |
| TidyTuesday Tweets | [12:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=775s) | Using `anti_join` function to filter out stop words (e.g., "and", "or", "the") from tokenized data frame |
| TidyTuesday Tweets | [14:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=885s) | Calculating summary statistics per word (average retweets and likes), then looking at distributions |
| TidyTuesday Tweets | [16:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=960s) | Explanation of Poisson log normal distribution (number of retweets fits this distribution) |
| TidyTuesday Tweets | [17:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1065s) | Additional example of Poisson log normal distribution (number of likes) |
| TidyTuesday Tweets | [18:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1100s) | Explanation of geometric mean as better summary statistic than median or arithmetic mean |
| TidyTuesday Tweets | [25:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1520s) | Using `floor_date` function from `lubridate` package to floor dates to the week level and tweaking so that a week starts on Monday (default is Sunday) |
| TidyTuesday Tweets | [30:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1820s) | Asking, "What topic is each week about?" using just the tweet text |
| TidyTuesday Tweets | [31:30](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1890s) | Calculating TF-IDF of tweets, with week as the "document" |
| TidyTuesday Tweets | [33:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2025s) | Using `top_n` and `group_by` functions to select the top tf-idf score for each week |
| TidyTuesday Tweets | [37:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2275s) | Using `str_detect` function to filter out "words" that are just numbers (e.g., 16, 36) |
| TidyTuesday Tweets | [41:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2460s) | Using `distinct` function with .keep_all argument to ensure only top 1 result, as alternative to `top_n` function (which includes ties) |
| TidyTuesday Tweets | [42:30](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2550s) | Making Jenny Bryan disappointed |
| TidyTuesday Tweets | [42:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2575s) | Using `geom_text` function to add text labels to graph to show to word associated with each week |
| TidyTuesday Tweets | [44:10](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2650s) | Using `geom_text_repel` function from `ggrepel` package as an alternative to `geom_text` function for adding text labels to graph |
| TidyTuesday Tweets | [46:30](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2790s) | Using `rvest` package to scrape web data from a table in Tidy Tuesday README |
| TidyTuesday Tweets | [51:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3060s) | Starting to look at #rstats tweets |
| TidyTuesday Tweets | [56:35](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3395s) | Spotting signs of fake accounts with purchased followers (lots of hashtags) |
| TidyTuesday Tweets | [59:15](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3555s) | Explanation of spotting fake accounts |
| TidyTuesday Tweets | [1:00:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3645s) | Using `str_detect` to filter out web URLs |
| TidyTuesday Tweets | [1:03:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3835s) | Using `str_count` function and some regex to count how many hashtags a tweet has |
| TidyTuesday Tweets | [1:07:25](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4045s) | Creating a Bland-Altman plot (total on x-axis, variable of interest on y-axis) |
| TidyTuesday Tweets | [1:08:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4125s) | Using `geom_text` function with check_overlap argument to add labels to scatterplot |
| TidyTuesday Tweets | [1:12:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4340s) | Asking, "Who are the most active #rstats tweeters?" |
| TidyTuesday Tweets | [1:15:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4500s) | Summary of screncast |



***



#### US Dairy Consumption

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US Dairy Consumption | [2:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=170s) | Identifying the need for a gather step |
| US Dairy Consumption | [4:40](https://www.youtube.com/watch?v=13iG_HkEPVc&t=280s) | Changing snake case to title case using `str_to_title` and `str_replace_all` functions |
| US Dairy Consumption | [6:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=380s) | Identifying need for separating categories into major and minor categories (e.g., "Cheese Other" can be divided into "Cheese" and "Other") |
| US Dairy Consumption | [7:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=430s) | Using `separate` function to split categories into major and minor categories (good explanation of "extra" argument, which merges additional separations into one field) |
| US Dairy Consumption | [8:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=500s) | Using `coalesce` function to deal with NAs resulting from above step |
| US Dairy Consumption | [10:30](https://www.youtube.com/watch?v=13iG_HkEPVc&t=630s) | Dealing with graph of minor category that is linked to multiple major categories ("Other" linked to "Cheese" and "Frozen") |
| US Dairy Consumption | [13:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=790s) | Introducing `fct_lump` function as an approach to work with many categories |
| US Dairy Consumption | [14:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=890s) | Introducing facetting (`facet_wrap` function) as second alternative to working with many categories |
| US Dairy Consumption | [15:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=950s) | Dealing with "Other" category having two parts to it by using `ifelse` function in the cleaning step (e.g., go from "Other" to "Other Cheese") |
| US Dairy Consumption | [19:45](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1185s) | Looking at page for the `sweep` package |
| US Dairy Consumption | [21:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1280s) | Using `tk_ts` function to coerce a tibble to a timeseries |
| US Dairy Consumption | [22:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1330s) | Turning year column (numeric) into a date by adding number of years to Jan 1, 0001 |
| US Dairy Consumption | [26:00](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1560s) | Nesting time series object into each combination of category and product |
| US Dairy Consumption | [27:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1670s) | Applying ETS (Error, Trend, Seasonal) model to each time series |
| US Dairy Consumption | [28:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1690s) | Using `sw_glance` function (`sweep` package's version of `glance` function) to pull out model parameters from model field created in above step |
| US Dairy Consumption | [29:45](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1785s) | Using `sw_augment` function to append fitted values and residuals from the model to the original data |
| US Dairy Consumption | [30:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1850s) | Visualising actual and fitted values on the same graph to get a look at the ETS model |
| US Dairy Consumption | [32:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1930s) | Using `Arima` function (note the capital A) as alternative to ETS (not sure what difference is between `arima` and `Arima`) |
| US Dairy Consumption | [35:00](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2100s) | Forecasting into the future using an ETS model using various functions: `unnest`, `sw_sweep`, `forecast` |
| US Dairy Consumption | [37:45](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2265s) | Using `geom_ribbon` function to add confidence bounds to forecast |
| US Dairy Consumption | [40:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2420s) | Forecasting using auto-ARIMA (instead of ETS) |
| US Dairy Consumption | [40:55](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2455s) | Applying two forecasting methods at the same time (auto-ARIMA and ETS) using the `crossing` function |
| US Dairy Consumption | [41:55](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2515s) | Quick test of how `invoke` function works (used to call a function easily, e.g., when it is a character string instead of called directly) |
| US Dairy Consumption | [47:35](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2855s) | Removing only one part of legend (line type of solid or dashed) using `scale_linetype_discrete` function |
| US Dairy Consumption | [51:25](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3085s) | Using `gather` function to clean up new dataset |
| US Dairy Consumption | [52:05](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3125s) | Using `fct_recode` to fix a typo in a categorical variable |
| US Dairy Consumption | [56:00](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3360s) | Copy-pasting previous forecasting code to cheese and reviewing any changes needed |
| US Dairy Consumption | [57:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3440s) | Discussing alternative approach: creating interactive visualisation using `shiny` package to do direct comparisons |



***



#### US PhDs

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US PhDs | [3:15](https://www.youtube.com/watch?v=KzRP40PzopY&t=195s) | Using `read_xlsx` function to read in Excel spreadsheet, including skipping first few rows that don't have data |
| US PhDs | [7:25](https://www.youtube.com/watch?v=KzRP40PzopY&t=445s) | Overview of starting very messy data |
| US PhDs | [8:20](https://www.youtube.com/watch?v=KzRP40PzopY&t=500s) | Using `gather` function to clean up wide dataset |
| US PhDs | [9:20](https://www.youtube.com/watch?v=KzRP40PzopY&t=560s) | Using `fill` function to fill in NA values with a entries in a previous observation |
| US PhDs | [10:10](https://www.youtube.com/watch?v=KzRP40PzopY&t=610s) | Cleaning variable that has number and percent in it, on top of one another using a combination of `ifelse` and `fill` functions |
| US PhDs | [12:00](https://www.youtube.com/watch?v=KzRP40PzopY&t=720s) | Using `spread` function on cleaned data to separate number and percent by year |
| US PhDs | [13:50](https://www.youtube.com/watch?v=KzRP40PzopY&t=830s) | Spotted a mistake where he had the wrong string on `str_detect` function |
| US PhDs | [16:50](https://www.youtube.com/watch?v=KzRP40PzopY&t=1010s) | Using `sample` function to get 6 random fields of study to graph |
| US PhDs | [18:50](https://www.youtube.com/watch?v=KzRP40PzopY&t=1130s) | Cleaning another dataset, which is much easier to clean |
| US PhDs | [19:05](https://www.youtube.com/watch?v=KzRP40PzopY&t=1145s) | Renaming the first field, even without knowing the exact name |
| US PhDs | [21:55](https://www.youtube.com/watch?v=KzRP40PzopY&t=1315s) | Cleaning another dataset |
| US PhDs | [23:10](https://www.youtube.com/watch?v=KzRP40PzopY&t=1390s) | Discussing challenge of when indentation is used in original dataset (for group / sub-group distinction) |
| US PhDs | [25:20](https://www.youtube.com/watch?v=KzRP40PzopY&t=1520s) | Starting to separate out data that is appended to one another in the original dataset (all, male, female) |
| US PhDs | [27:30](https://www.youtube.com/watch?v=KzRP40PzopY&t=1650s) | Removing field with long name using `contains` function |
| US PhDs | [28:10](https://www.youtube.com/watch?v=KzRP40PzopY&t=1690s) | Using `fct_recode` function to rename an oddly-named category in a categorical variable (`ifelse` function is probably a better alternative) |
| US PhDs | [35:30](https://www.youtube.com/watch?v=KzRP40PzopY&t=2130s) | Discussing solution to broad major field description and fine major field description (meaningfully indented in original data) |
| US PhDs | [39:40](https://www.youtube.com/watch?v=KzRP40PzopY&t=2380s) | Using `setdiff` function to separate broad and fine major fields |



***



#### French Train Delays

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| French Train Delays | [10:20](https://www.youtube.com/watch?v=bmaigtpKyiM&t=620s) | Boxplots of departure stations using `fct_lump` function |
| French Train Delays | [14:25](https://www.youtube.com/watch?v=bmaigtpKyiM&t=865s) | Creating heat map of departure and arrival delays, then cleaning up a sparse heat map |
| French Train Delays | [15:30](https://www.youtube.com/watch?v=bmaigtpKyiM&t=930s) | Using `fct_reorder` function and length function to reorder stations based on how frequently they appear |
| French Train Delays | [16:30](https://www.youtube.com/watch?v=bmaigtpKyiM&t=990s) | Using `fct_infreq` to reorder based on infrequently-appearing stations (same as above, but without a trick needed) |
| French Train Delays | [17:45](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1065s) | Using `fct_lump` function to lump based on proportion instead of number of top categories desired |
| French Train Delays | [18:45](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1125s) | Using `scale_fill_gradient2` function to specify diverging colour scale |
| French Train Delays | [26:00](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1560s) | Checking another person's take on the data, which is a heatmap over time |
| French Train Delays | [28:40](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1720s) | Converting year and month (as digits) into date-class variable using `sprintf` function and padding month number with extra zero when necessary |
| French Train Delays | [34:50](https://www.youtube.com/watch?v=bmaigtpKyiM&t=2090s) | Using `summarise_at` function to quickly sum multiple columns |
| French Train Delays | [39:35](https://www.youtube.com/watch?v=bmaigtpKyiM&t=2375s) | Creating heatmap using `geom_tile` function for percentage of late trains by station over time |
| French Train Delays | [45:05](https://www.youtube.com/watch?v=bmaigtpKyiM&t=2705s) | Using `fill` function to fill in missing NA values with data from previous observations |
| French Train Delays | [50:35](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3035s) | Grouping multiple variables into a single category using `paste0` function |
| French Train Delays | [51:40](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3100s) | Grouping heatmap into International / National chunks with a weird hack |
| French Train Delays | [52:20](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3140s) | Further separating International / National visually |
| French Train Delays | [53:30](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3210s) | Less hacky way of separating International / National (compared to previous two rows) |



***



#### Seattle Bike Counts

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Seattle Bike Counts | [6:15](https://www.youtube.com/watch?v=sBho2GJE5lc&t=375s) | Using `summarise_all` and `summarise_at` functions to aggregate multiple variables at the same time |
| Seattle Bike Counts | [8:15](https://www.youtube.com/watch?v=sBho2GJE5lc&t=495s) | Using magnitude instead of absolute numbers to see trends in time of day |
| Seattle Bike Counts | [12:00](https://www.youtube.com/watch?v=sBho2GJE5lc&t=720s) | Dividing time into categories (four categories for times of day, e.g., morning commute, night) using `between` function |
| Seattle Bike Counts | [15:00](https://www.youtube.com/watch?v=sBho2GJE5lc&t=900s) | Looking for systematically missing data (which would bias the results of the analysis) |
| Seattle Bike Counts | [19:45](https://www.youtube.com/watch?v=sBho2GJE5lc&t=1185s) | Summarising using a filter in the arguments based on whether the time window is during a commute time |
| Seattle Bike Counts | [22:45](https://www.youtube.com/watch?v=sBho2GJE5lc&t=1365s) | Combining day of week and hour using functions in the `lubridate` package and `as.difftime` function (but then he uses facetting as an easier method) |
| Seattle Bike Counts | [26:30](https://www.youtube.com/watch?v=sBho2GJE5lc&t=1590s) | Normalizing day of week data to percent of weekly traffic |
| Seattle Bike Counts | [42:00](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2520s) | Starting analysis of directions of travel by time of day (commute vs. reverse-commute) |
| Seattle Bike Counts | [43:45](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2625s) | Filtering out weekend days using wday function from `lubridate` package |
| Seattle Bike Counts | [45:30](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2730s) | Using `spread` function to create new variable of ratio of bike counts at different commute times |
| Seattle Bike Counts | [47:30](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2850s) | Visualizing ratio of bike counts by time of day |
| Seattle Bike Counts | [50:15](https://www.youtube.com/watch?v=sBho2GJE5lc&t=3015s) | Visualizing ratio by hour instead of time of day |
| Seattle Bike Counts | [52:50](https://www.youtube.com/watch?v=sBho2GJE5lc&t=3170s) | Ordering crossing in graph by when the average trip happened using mean of hour weighted by bike count |
| Seattle Bike Counts | [54:50](https://www.youtube.com/watch?v=sBho2GJE5lc&t=3290s) | Quick and dirty filter when creating a new variable within a `mutate` function |



***



#### Tennis Tournaments

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Tennis Tournaments | [5:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=300s) | Identifying duplicated rows ands fixing them |
| Tennis Tournaments | [11:15](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=675s) | Using `add_count` and `fct_reorder` functions to order categories that are broken down into sub-categories for graphing |
| Tennis Tournaments | [13:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=780s) | Tidying graph titles (e.g., replacing underscores with spaces) using `str_to_title` and `str_replace` functions |
| Tennis Tournaments | [15:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=900s) | Using `inner_join` function to merge datasets |
| Tennis Tournaments | [15:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=930s) | Calculating age from date of birth using `difftime` and `as.numeric` functions |
| Tennis Tournaments | [16:35](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=995s) | Adding simple calculations like `mean` and `median` into the text portion of markdown document |
| Tennis Tournaments | [17:45](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1065s) | Looking at distribution of wins by sex using overlapping histograms |
| Tennis Tournaments | [18:55](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1135s) | Binning years into decades using truncated division `%/%` |
| Tennis Tournaments | [20:15](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1215s) | Splitting up boxplots so that they are separated into pairs (M/F) across a different group (decade) using `interaction` function |
| Tennis Tournaments | [20:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1230s) | Analyzing distribution of ages across decades, looking specifically at the effect of Serena Williams (one individual having a disproportionate affect on the data, making it look like there's a trend) |
| Tennis Tournaments | [24:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1470s) | Avoiding double-counting of individuals by counting their average age instead of their age at each win |
| Tennis Tournaments | [30:20](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1820s) | Starting analysis to predict winner of Grand Slam tournaments |
| Tennis Tournaments | [35:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2100s) | Creating rolling count using `row_number` function to make a count of previous tournament experience |
| Tennis Tournaments | [39:45](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2385s) | Creating rolling win count using `cumsum` function |
| Tennis Tournaments | [41:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2460s) | Lagging rolling win count using `lag` function (otherwise we get information about a win before a player has actually won, for prediction purposes) |
| Tennis Tournaments | [43:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2610s) | Asking, "When someone is a finalist, what is their probability of winning as a function of previous tournaments won?" |
| Tennis Tournaments | [48:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2880s) | Asking, "How does the number of wins a finalist has affect their chance of winning?" |
| Tennis Tournaments | [49:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2940s) | Backtesting simple classifier where person with more tournament wins is predicted to win the given tournament |
| Tennis Tournaments | [51:45](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3105s) | Creating classifier that gives points based on how far a player got in previous tournaments |
| Tennis Tournaments | [52:55](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3175s) | Using `match` function to turn name of round reached (1st round, 2nd round, …) into a number score (1, 2, …) |
| Tennis Tournaments | [54:20](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3260s) | Using `cummean` function to get score of average past performance (instead of `cumsum` function) |
| Tennis Tournaments | [1:04:10](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3850s) | Pulling names of rounds (1st round, 2nd round, … ) based on the rounded numeric score of previous performance |



***



#### Bird Collisions

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Bird Collisions | [2:45](https://www.youtube.com/watch?v=zjWm__nFLXI&t=165s) | Analyzing when NAs appear in a dimension |
| Bird Collisions | [7:30](https://www.youtube.com/watch?v=zjWm__nFLXI&t=450s) | Looking at multiple categorical variable at the same time by gathering them into one column and eventually graphing each as a different facet |
| Bird Collisions | [9:30](https://www.youtube.com/watch?v=zjWm__nFLXI&t=570s) | Re-order facet graphs according to which ones have the fewest categories in them to ones that have the most |
| Bird Collisions | [20:45](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1245s) | Geometric mean for estimating counts when there are a lot of low values (1-3 bird collisions, in this case) |
| Bird Collisions | [23:15](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1395s) | Filling in "blank" observations where there were no observations made |
| Bird Collisions | [27:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1620s) | Using log+1 to convert a dimension with values of 0 into a log scale |
| Bird Collisions | [29:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1740s) | Adding confidence bounds for data using a geometric mean (where he first gets the idea of bootstrapping) |
| Bird Collisions | [32:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1920s) | Actual coding of bootstrap starts |
| Bird Collisions | [38:30](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2310s) | Adding confidence bounds using bootstrap data |
| Bird Collisions | [42:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2520s) | Investigating potential confounding variables |
| Bird Collisions | [44:15](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2655s) | Discussing approaches to dealing with confounding variables |
| Bird Collisions | [46:45](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2805s) | Using `complete` function to get explicit NA values |



***



#### Student-Teacher Rations

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Student-Teacher Ratios | [7:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=450s) | Using `slice` function to select 10 highest and 10 lowest student-teacher ratios (like a filter using row numbers) |
| Student-Teacher Ratios | [12:35](https://www.youtube.com/watch?v=NoUHdrailxA&t=755s) | Adding GDP per capita to a dataset using `WDI` package |
| Student-Teacher Ratios | [17:40](https://www.youtube.com/watch?v=NoUHdrailxA&t=1060s) | Using `geom_text` to add labels to points on a scatterplot |
| Student-Teacher Ratios | [19:00](https://www.youtube.com/watch?v=NoUHdrailxA&t=1140s) | Using `WDIsearch` function from `WDI` package to search for country population data |
| Student-Teacher Ratios | [23:20](https://www.youtube.com/watch?v=NoUHdrailxA&t=1400s) | Explanation of trick with `geom_text` function's check_overlap argument to get label for US to appear by rearranging row order |
| Student-Teacher Ratios | [25:45](https://www.youtube.com/watch?v=NoUHdrailxA&t=1545s) | Using `comma_format` function from `scales` format to get more readable numeric legend (e.g., "500,000,000" instead of "5e+08") |
| Student-Teacher Ratios | [27:55](https://www.youtube.com/watch?v=NoUHdrailxA&t=1675s) | Exploring different education-related indicators in the `WDI` package |
| Student-Teacher Ratios | [31:55](https://www.youtube.com/watch?v=NoUHdrailxA&t=1915s) | Using `spread` function (now `pivot_wider`) to turn data from tidy to wide format |
| Student-Teacher Ratios | [32:15](https://www.youtube.com/watch?v=NoUHdrailxA&t=1935s) | Using `to_snake_case` function from `snakecase` package to convert field names to snake_case |
| Student-Teacher Ratios | [48:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=2910s) | Exploring female/male school secondary school enrollment |
| Student-Teacher Ratios | [51:50](https://www.youtube.com/watch?v=NoUHdrailxA&t=3110s) | Note of caution on keeping confounders in mind when interpreting scatterplots |
| Student-Teacher Ratios | [52:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=3150s) | Creating a linear regression of secondary school enrollment to explore confounders |
| Student-Teacher Ratios | [54:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=3270s) | Discussing the actual confounder (GDP per capita) in the linear regression above |
| Student-Teacher Ratios | [57:20](https://www.youtube.com/watch?v=NoUHdrailxA&t=3440s) | Adding world region as another potential confounder |
| Student-Teacher Ratios | [58:00](https://www.youtube.com/watch?v=NoUHdrailxA&t=3480s) | Using `aov` function (ANOVA) to explore confounders further |
| Student-Teacher Ratios | [1:06:50](https://www.youtube.com/watch?v=NoUHdrailxA&t=4010s) | Reviewing and interpreting the final linear regression model |
| Student-Teacher Ratios | [1:08:00](https://www.youtube.com/watch?v=NoUHdrailxA&t=4080s) | Using `cor` function (correlation) to get correlation matrix for three variables (and brief explanation of multi-collinearity) |
| Student-Teacher Ratios | [1:10:10](https://www.youtube.com/watch?v=NoUHdrailxA&t=4210s) | Summary of screencast |



***



#### Plastic Waste

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Plastic Waste | [1:45](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=105s) | Using `summarise_all` to get proportion of NA values across many variables |
| Plastic Waste | [16:50](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1010s) | Adding text labels to scatter plot for some points using check_overlap argument |
| Plastic Waste | [21:45](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1305s) | Using `pmin` function to get the lower of two possible numbers for a percentage variable that was showing > 100% |
| Plastic Waste | [29:00](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1740s) | Starting to make a choropleth map |
| Plastic Waste | [29:30](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1770s) | Connecting ISO country names (used in mapping code) to country names given in the dataset |
| Plastic Waste | [32:00](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1920s) | Actual code to create the map using given longitude and latitude |
| Plastic Waste | [33:45](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2025s) | Using `fuzzyjoin` package to link variables that use regular expression instead of character (using `regex_right_join` / `regex_left_join` function) |
| Plastic Waste | [36:15](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2175s) | Using `coord_fixed` function as a hack to get proper ratios for maps |
| Plastic Waste | [39:30](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2370s) | Bringing in additional data using `WDI` package |
| Plastic Waste | [47:30](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2850s) | Using `patchwork` package to show multiple graphs in the same plot |
| Plastic Waste | [53:00](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=3180s) | Importing and renaming multiple indicators from the `WDI` package at the same time |



***



#### Wine Ratings

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Wine Ratings | [3:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=195s) | Using `extract` function from `tidyr` package to pull out year from text field |
| Wine Ratings | [9:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=555s) | Changing `extract` function to pull out year column more accurately |
| Wine Ratings | [13:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=780s) | Starting to explore prediction of points |
| Wine Ratings | [17:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1020s) | Using `fct_lump` on country variable to collapse countries into an "Other" category, then `fct_relevel` to set the baseline category for a linear model |
| Wine Ratings | [21:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1290s) | Investigating year as a potential confounding variable |
| Wine Ratings | [24:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1485s) | Investigating "taster_name" as a potential confounding variable |
| Wine Ratings | [27:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1665s) | Coefficient (TIE fighter) plot to see effect size of terms in a linear model, using `tidy` function from `broom` package |
| Wine Ratings | [30:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1845s) | Polishing category names for presentation in graph using `str_replace` function |
| Wine Ratings | [32:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1935s) | Using `augment` function to add predictions of linear model to original data |
| Wine Ratings | [33:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2010s) | Plotting predicted points vs. actual points |
| Wine Ratings | [34:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2085s) | Using ANOVA to determine the amount of variation that explained by different terms |
| Wine Ratings | [36:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2205s) | Using `tidytext` package to set up wine review text for Lasso regression |
| Wine Ratings | [40:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2400s) | Setting up and using `pairwise_cor` function to look at words that appear in reviews together |
| Wine Ratings | [45:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2700s) | Creating sparse matrix using `cast_sparse` function from `tidytext` package; used to perform a regression on positive/negative words |
| Wine Ratings | [46:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2805s) | Checking if row names of sparse matrix correspond to the wine_id values they represent |
| Wine Ratings | [47:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2820s) | Setting up sparse matrix for using `glmnet` package to do sparse regression using Lasso method |
| Wine Ratings | [48:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2895s) | Actually writing code for doing Lasso regression |
| Wine Ratings | [49:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2985s) | Basic explanation of Lasso regression |
| Wine Ratings | [51:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3060s) | Putting Lasso model into tidy format |
| Wine Ratings | [53:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3195s) | Explaining how the number of terms increases as lambda (penalty parameter) decreases |
| Wine Ratings | [54:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3240s) | Answering how we choose a lambda value (penalty parameter) for Lasso regression |
| Wine Ratings | [56:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3405s) | Using parallelization for intensive computations |
| Wine Ratings | [58:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3510s) | Adding price (from original linear model) to Lasso regression |
| Wine Ratings | [1:02:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3735s) | Shows glmnet.fit piece of a Lasso model (using `glmnet` package) |
| Wine Ratings | [1:03:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3810s) | Picking a lambda value (penalty parameter) and explaining which one to pick |
| Wine Ratings | [1:08:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4095s) | Taking most extreme coefficients (positive and negative) by grouping theme by direction |
| Wine Ratings | [1:10:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4230s) | Demonstrating `tidytext` package's sentiment lexicon, then looking at individual reviews to demonstrate the model |
| Wine Ratings | [1:17:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4650s) | Visualizing each coefficient's effect on a single review |
| Wine Ratings | [1:20:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4830s) | Using `str_trunc` to truncate character strings |



***



#### Ramen Reviews

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Ramen Reviews | [1:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=105s) | Looking at the website the data came from |
| Ramen Reviews | [2:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=175s) | Using `gather` function (now `pivot_longer`) to convert wide data to long (tidy) format |
| Ramen Reviews | [4:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=255s) | Graphing counts of all categorical variables at once, then exploring them |
| Ramen Reviews | [5:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=335s) | Using `fct_lump` function to lump three categorical variables to the top N categories and "Other" |
| Ramen Reviews | [7:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=465s) | Using `reorder_within` function to re-order factors that have the same name across multiple facets |
| Ramen Reviews | [9:10](https://www.youtube.com/watch?v=tCa2di7aEP4&t=550s) | Using `lm` function (linear model) to predict star rating |
| Ramen Reviews | [9:50](https://www.youtube.com/watch?v=tCa2di7aEP4&t=590s) | Visualising effects (and 95% CI) of indendent variables in linear model with a coefficient plot (TIE fighter plot) |
| Ramen Reviews | [11:30](https://www.youtube.com/watch?v=tCa2di7aEP4&t=690s) | Using `fct_relevel` function to get "Other" as the base reference level for categorical independent variables in a linear model |
| Ramen Reviews | [13:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=785s) | Using `extract` function and regex to split a camelCase variable into two separate variables |
| Ramen Reviews | [14:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=885s) | Using `facet_wrap` function to split coefficient / TIE fighter plot into three separate plots, based on type of coefficient |
| Ramen Reviews | [15:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=940s) | Using `geom_vline` function to add reference line to graph |
| Ramen Reviews | [17:20](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1040s) | Using `unnest_tokens` function from `tidytext` package to explore the relationship between variety (a sparse categorical variable) and star rating |
| Ramen Reviews | [18:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1135s) | Explanation of how he would approach variety variable with Lasso regression |
| Ramen Reviews | [19:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1175s) | Web scraping the using `rvest` package and `SelectorGadget` (Chrome Extension CSS selector) |
| Ramen Reviews | [21:20](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1280s) | Actually writing code for web scraping, using `read_html`, `html_node`, and `html_table` functions |
| Ramen Reviews | [22:25](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1345s) | Using `clean_names` function from `janitor` package to clean up names of variables |
| Ramen Reviews | [23:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1385s) | Explanation of web scraping task: get full review text using the links from the review summary table scraped above |
| Ramen Reviews | [25:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1540s) | Using `parse_number` function as alternative to `as.integer` function to cleverly drop extra weird text in review number |
| Ramen Reviews | [26:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1605s) | Using `SelectorGadget` (Chrome Extension CSS selector) to identify part of page that contains review text |
| Ramen Reviews | [27:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1655s) | Using `html_nodes`, `html_text`, and `str_subset` functions to write custom function to scrape review text identified in step above |
| Ramen Reviews | [29:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1755s) | Adding `message` function to custom scraping function to display URLs as they are being scraped |
| Ramen Reviews | [30:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1815s) | Using `unnest_tokens` and `anti_join` functions to split review text into individual words and remove stop words (e.g., "the", "or", "and") |
| Ramen Reviews | [31:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1865s) | Catching a mistake in the custom function causing it to read the same URL every time |
| Ramen Reviews | [31:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1915s) | Using `str_detect` function to filter out review paragraphs without a keyword in it |
| Ramen Reviews | [32:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1960s) | Using `str_remove` function and regex to get rid of string that follows a specific pattern |
| Ramen Reviews | [34:10](https://www.youtube.com/watch?v=tCa2di7aEP4&t=2050s) | Explanation of `possibly` and `safely` functions in `purrr` package |
| Ramen Reviews | [37:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=2265s) | Reviewing output of the URL that failed to scrape, including using `character(0)` as a default null value |
| Ramen Reviews | [48:00](https://www.youtube.com/watch?v=tCa2di7aEP4&t=2880s) | Using `pairwise_cor` function from `widyr` package to see which words tend to appear in reviews together |
| Ramen Reviews | [51:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3065s) | Using `igraph` and `ggraph` packages to make network plot of word correlations |
| Ramen Reviews | [51:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3115s) | Using `geom_node_text` function to add labels to network plot |
| Ramen Reviews | [52:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3155s) | Including all words (not just those connected to others) as vertices in the network plot |
| Ramen Reviews | [54:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3280s) | Tweaking and refining network plot aesthetics (vertex size and colour) |
| Ramen Reviews | [56:00](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3360s) | Weird hack for getting a dark outline on hard-to-see vertex points |
| Ramen Reviews | [59:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3555s) | Summary of screencast |



***



#### Media Franchise Revenue

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Media Franchise Revenue | [9:15](https://www.youtube.com/watch?v=1xsbTs9-a50&t=555s) | Explaining use of `semi_join` function to aggregate and filter groups |
| Media Franchise Revenue | [11:00](https://www.youtube.com/watch?v=1xsbTs9-a50&t=660s) | Putting the largest categories on the bottom of a stacked bar chart |
| Media Franchise Revenue | [14:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=870s) | Using `glue` function as alternative to `paste` for combining text, plus good explanation of it |
| Media Franchise Revenue | [19:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1170s) | Multiple re-ordering using `fct_reorder` function of facetted graph (he works through several obstacles) |
| Media Franchise Revenue | [20:40](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1240s) | Re-ordering the position of facetted graphs so that highest total revenue is at top left |
| Media Franchise Revenue | [26:00](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1560s) | Investigating relationship between year created and revenue |
| Media Franchise Revenue | [26:40](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1600s) | Creating scatter plot with points scaled by size and labelled points (`geom_text` function) |
| Media Franchise Revenue | [29:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1770s) | Summary of screencast up to this point |
| Media Franchise Revenue | [29:50](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1790s) | Starting analysis original media of franchise (e.g., novel, video game, animated film) and revenue type (e.g., box office, merchandise) |
| Media Franchise Revenue | [33:35](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2015s) | Graphing original media and revenue category as facetted bar plot with lots of reordering (ends at around 38:40) |
| Media Franchise Revenue | [40:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2430s) | Alternative visualization of original media/revenue category using heat map |
| Media Franchise Revenue | [41:20](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2480s) | Using `scale_fill_gradient2` function to specify custom colour scale |
| Media Franchise Revenue | [42:05](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2525s) | Getting rid of gridlines in graph using `theme` function's panel.grid argument |
| Media Franchise Revenue | [44:05](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2645s) | Using `fct_rev` function to reverse levels of factors |
| Media Franchise Revenue | [44:35](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2675s) | Fixing overlapping axis text with tweaks to `theme` function's axis.text argument |
| Media Franchise Revenue | [46:05](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2765s) | Reviewing visualization that inspired this dataset |
| Media Franchise Revenue | [47:25](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2845s) | Adding text of total revenue to the end of each bar in a previous graph |
| Media Franchise Revenue | [50:20](https://www.youtube.com/watch?v=1xsbTs9-a50&t=3020s) | Using `paste0` function at add a "B" (for "billions") to the end of text labels on graph |
| Media Franchise Revenue | [51:35](https://www.youtube.com/watch?v=1xsbTs9-a50&t=3095s) | Using `expand_limits` functions to give more space for text labels not to get cut off |
| Media Franchise Revenue | [53:45](https://www.youtube.com/watch?v=1xsbTs9-a50&t=3225s) | Summary of screencast |



***



#### Women's World Cup

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Women's World Cup | [2:15](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=135s) | Adding country names using `countrycode` package |
| Women's World Cup | [3:45](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=225s) | Web scraping country codes from Wikipedia |
| Women's World Cup | [6:00](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=360s) | Combining tables that are separate lists into one dataframe |
| Women's World Cup | [14:00](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=840s) | Using `rev` function (reverse) to turn multiple rows of soccer match scores into one row (base team and opposing team) |
| Women's World Cup | [26:30](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=1590s) | Applying a `geom_smooth` linear model line to a scatter plot, then facetting it |
| Women's World Cup | [28:30](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=1710s) | Adding a line with a slope of 1 (x = y) using `geom_abline` |
| Women's World Cup | [40:00](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=2400s) | Pulling out elements of a list that is embedded in a dataframe |
| Women's World Cup | [1:09:45](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=4185s) | Using `glue` function to add context to facet titles |



***



#### Bob Ross Paintings

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Bob Ross Paintings | [1:40](https://www.youtube.com/watch?v=sD993H5FBIY&t=100s) | Using `clean_names` function in `janitor` package to get field names to snake_case |
| Bob Ross Paintings | [1:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=110s) | Using `gather` function (now 'pivot_longer') to get wide elements into tall (tidy) format |
| Bob Ross Paintings | [2:35](https://www.youtube.com/watch?v=sD993H5FBIY&t=155s) | Cleaning text (`str_to_title`, `str_replace`) to get into nicer-to-read format |
| Bob Ross Paintings | [3:30](https://www.youtube.com/watch?v=sD993H5FBIY&t=210s) | Using `str_remove_all` function to trim trimming quotation marks and backslashes |
| Bob Ross Paintings | [4:40](https://www.youtube.com/watch?v=sD993H5FBIY&t=280s) | Using `extract` function to extract the season number and episode number from episode field; uses regex capturing groups |
| Bob Ross Paintings | [14:00](https://www.youtube.com/watch?v=sD993H5FBIY&t=840s) | Using `add_count` function's name argument to specify field's name |
| Bob Ross Paintings | [15:35](https://www.youtube.com/watch?v=sD993H5FBIY&t=935s) | Getting into whether the elements of Ross's paintings changed over time (e.g., are mountains more/less common over time?) |
| Bob Ross Paintings | [20:00](https://www.youtube.com/watch?v=sD993H5FBIY&t=1200s) | Quick point: could have used logistic regression to see change over time of elements |
| Bob Ross Paintings | [21:10](https://www.youtube.com/watch?v=sD993H5FBIY&t=1270s) | Asking, "What elements tends to appear together?" prompting clustering analysis |
| Bob Ross Paintings | [22:15](https://www.youtube.com/watch?v=sD993H5FBIY&t=1335s) | Using `pairwise_cor` to see which elements tend to appear together |
| Bob Ross Paintings | [22:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=1370s) | Discussion of a blind spot of pairwise correlation (high or perfect correlation on elements that only appear once or twice) |
| Bob Ross Paintings | [28:05](https://www.youtube.com/watch?v=sD993H5FBIY&t=1685s) | Asking, "What are clusters of elements that belong together?" |
| Bob Ross Paintings | [28:30](https://www.youtube.com/watch?v=sD993H5FBIY&t=1710s) | Creating network plot using `ggraph` and `igraph` packages |
| Bob Ross Paintings | [30:15](https://www.youtube.com/watch?v=sD993H5FBIY&t=1815s) | Reviewing network plot for interesting clusters (e.g., beach cluster, mountain cluster, structure cluster) |
| Bob Ross Paintings | [31:55](https://www.youtube.com/watch?v=sD993H5FBIY&t=1915s) | Explanation of Principal Component Analysis (PCA) |
| Bob Ross Paintings | [34:35](https://www.youtube.com/watch?v=sD993H5FBIY&t=2075s) | Start of actual PCA coding |
| Bob Ross Paintings | [34:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=2090s) | Using `acast` function to create matrix of painting titles x painting elements (initially wrong, corrected at 36:30) |
| Bob Ross Paintings | [36:55](https://www.youtube.com/watch?v=sD993H5FBIY&t=2215s) | Centering the matrix data using `t` function (transpose of matrix), `colSums` function, and `colMeans` functions |
| Bob Ross Paintings | [38:15](https://www.youtube.com/watch?v=sD993H5FBIY&t=2295s) | Using `svd` function to performn singular value decomposition, then tidying with `broom` package |
| Bob Ross Paintings | [39:55](https://www.youtube.com/watch?v=sD993H5FBIY&t=2395s) | Exploring one principal component to get a better feel for what PCA is doing |
| Bob Ross Paintings | [43:20](https://www.youtube.com/watch?v=sD993H5FBIY&t=2600s) | Using `reorder_within` function to re-order factors within a grouping |
| Bob Ross Paintings | [48:00](https://www.youtube.com/watch?v=sD993H5FBIY&t=2880s) | Exploring different matrix names in PCA (u, v, d) |
| Bob Ross Paintings | [56:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=3410s) | Looking at top 6 principal components of painting elements |
| Bob Ross Paintings | [57:45](https://www.youtube.com/watch?v=sD993H5FBIY&t=3465s) | Showing percentage of variation that each principal component is responsible for |



***



#### Pizza Ratings

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Pizza Ratings | [4:45](https://www.youtube.com/watch?v=Mkac8DHScps&t=285s) | Transforming time into something more readable (from time value of seconds since Unix epoch 1970-01-01), then converting it into a date |
| Pizza Ratings | [9:05](https://www.youtube.com/watch?v=Mkac8DHScps&t=545s) | Formatting x-axis text so that it is rotated and readable, then re-ordering using `fct_relevel` function so that it is in its proper ordinal order |
| Pizza Ratings | [11:00](https://www.youtube.com/watch?v=Mkac8DHScps&t=660s) | Converting string answers to integer counterparts to get an overall numeric value for how good each place is |
| Pizza Ratings | [12:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=750s) | Commentary on speed of `mutate` calculation within or without a group (non-grouped is slightly faster) |
| Pizza Ratings | [15:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=930s) | Re-ordering groups by total votes using `fct_reorder` function, while still maintaining the groups themselves |
| Pizza Ratings | [19:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=1155s) | Using `glue` package to combine place name and total respondents |
| Pizza Ratings | [20:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=1230s) | Using statistical test to give confidence intervals on average score |
| Pizza Ratings | [22:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=1335s) | Actually using the `t.test` function with toy example |
| Pizza Ratings | [23:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=1395s) | Using weighted linear model instead (which doesn't end up working) |
| Pizza Ratings | [26:00](https://www.youtube.com/watch?v=Mkac8DHScps&t=1560s) | Using custom function with `rep` function to get vector of repeated scores (sneaky way of weighting) so that we can perform a proper t-test |
| Pizza Ratings | [27:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=1650s) | Summarizing `t.test` function into a list (alternative to nesting) |
| Pizza Ratings | [31:20](https://www.youtube.com/watch?v=Mkac8DHScps&t=1880s) | Adding error bars using `geom_errorbarh` to make a TIE fighter plot that shows confidence intervals |
| Pizza Ratings | [36:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=2190s) | Bringing in additional data from Barstool ratings (to supplement survey of Open R meetup NY) |
| Pizza Ratings | [39:45](https://www.youtube.com/watch?v=Mkac8DHScps&t=2385s) | Getting survey data to the place level so that we can add an additional dataset |
| Pizza Ratings | [41:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=2475s) | Checking for duplicates in the joined data |
| Pizza Ratings | [42:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=2535s) | Calling off the planned analysis due to low sample sizes (too much noise, not enough overlap between datasets) |
| Pizza Ratings | [45:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=2715s) | Looking at Barstool data on its own |
| Pizza Ratings | [55:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=3315s) | Renaming all variables with a certain string pattern in them |
| Pizza Ratings | [58:00](https://www.youtube.com/watch?v=Mkac8DHScps&t=3480s) | Comparing Dave's reviews with all other critics |
| Pizza Ratings | [59:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=3555s) | Adding `geom_abline` showing x = y as comparison for `geom_smooth` linear model line |
| Pizza Ratings | [1:02:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=3750s) | Changing the location of the `aes` function to change what the legend icons look like for size aesthetic |



***



#### Car Fuel Efficiency

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Car Fuel Efficiency | [3:20](https://www.youtube.com/watch?v=RpeioixHOHw&t=200s) | Using `select`, `sort`, and `colnames` functions to sort variables in alphabetical order |
| Car Fuel Efficiency | [10:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=600s) | Adding `geom_abline` for y = x to a scatter plot for comparison |
| Car Fuel Efficiency | [18:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=1080s) | Visualising using `geom_boxplot` for mpg by vehicle class (size of car) |
| Car Fuel Efficiency | [24:45](https://www.youtube.com/watch?v=RpeioixHOHw&t=1485s) | Start of explanation of prediction goals |
| Car Fuel Efficiency | [27:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=1620s) | Creating train and test sets, along with trick using `sample_frac` function to randomly re-arrange all rows in a dataset |
| Car Fuel Efficiency | [28:35](https://www.youtube.com/watch?v=RpeioixHOHw&t=1715s) | First step of developing linear model: visually adding `geom_smooth` |
| Car Fuel Efficiency | [30:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=1800s) | Using `augment` function to add extra variables from model to original dataset (fitted values and residuals, especially) |
| Car Fuel Efficiency | [30:45](https://www.youtube.com/watch?v=RpeioixHOHw&t=1845s) | Creating residuals plot and explaining what you want and don't want to see |
| Car Fuel Efficiency | [31:50](https://www.youtube.com/watch?v=RpeioixHOHw&t=1910s) | Explanation of splines |
| Car Fuel Efficiency | [33:30](https://www.youtube.com/watch?v=RpeioixHOHw&t=2010s) | Visualising effect of regressing using natural splines |
| Car Fuel Efficiency | [35:10](https://www.youtube.com/watch?v=RpeioixHOHw&t=2110s) | Creating a tibble to test different degrees of freedom (1:10) for natural splines |
| Car Fuel Efficiency | [36:30](https://www.youtube.com/watch?v=RpeioixHOHw&t=2190s) | Using `unnest` function to get tidy versions of different models |
| Car Fuel Efficiency | [37:55](https://www.youtube.com/watch?v=RpeioixHOHw&t=2275s) | Visualising fitted values of all 6 different models at the same time |
| Car Fuel Efficiency | [42:10](https://www.youtube.com/watch?v=RpeioixHOHw&t=2530s) | Investigating whether the model got "better" as we added degrees of freedom to the natural splines, using the `glance` function |
| Car Fuel Efficiency | [47:45](https://www.youtube.com/watch?v=RpeioixHOHw&t=2865s) | Using ANOVA to perform a statistical test on whether natural splines as a group explain variation in MPG |
| Car Fuel Efficiency | [48:30](https://www.youtube.com/watch?v=RpeioixHOHw&t=2910s) | Exploring colinearity of dependant variables (displacement and cylinders) |
| Car Fuel Efficiency | [55:10](https://www.youtube.com/watch?v=RpeioixHOHw&t=3310s) | Binning years into every two years using `floor` function |
| Car Fuel Efficiency | [56:40](https://www.youtube.com/watch?v=RpeioixHOHw&t=3400s) | Using `summarise_at` function to do quick averaging of multiple variables |



***



#### Horror Movies

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Horror Movies | [4:15](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=255s) | Extracting digits (release year) from character string using regex, along with good explanation of `extract` function |
| Horror Movies | [8:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=480s) | Quick check on why `parse_number` is unable to parse some values -- is it because they are NA or some other reason? |
| Horror Movies | [9:45](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=585s) | Visually investigating correlation between budget and rating |
| Horror Movies | [11:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=710s) | Investigating correlation between MPAA rating (PG-13, R, etc.) and rating using boxplots |
| Horror Movies | [12:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=770s) | Using `pull` function to quickly check levels of a factor |
| Horror Movies | [13:30](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=810s) | Using ANOVA to check difference of variation within groups (MPAA rating) than between groups |
| Horror Movies | [15:40](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=940s) | Separating genre using `separate_rows` function (instead of `str_split` and `unnest`) |
| Horror Movies | [18:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1080s) | Removing boilerplate "Directed by..." and "With..." part of plot variable and isolating plot, first using regex, then by using `separate` function with periods as separator |
| Horror Movies | [20:40](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1240s) | Unnesting word tokens, removing stop words, and counting appearances |
| Horror Movies | [21:20](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1280s) | Aggregating by word to find words that appear in high- or low-rated movies |
| Horror Movies | [23:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1380s) | Discussing potential confounding factors for ratings associated with specific words |
| Horror Movies | [24:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1490s) | Searching for duplicated movie titles |
| Horror Movies | [25:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1550s) | De-duping using `distinct` function |
| Horror Movies | [26:55](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1615s) | Loading in and explaining `glmnet` package |
| Horror Movies | [28:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1680s) | Using movie titles to pull out ratings using `rownmaes` and `match` functions to create an index of which rating to pull out of the original dataset |
| Horror Movies | [29:10](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1750s) | Actually using `glmnet` function to create lasso model |
| Horror Movies | [34:05](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2045s) | Showing built-in plot of lasso lambda against mean-squared error |
| Horror Movies | [37:05](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2225s) | Explaining when certain terms appeared in the lasso model as the lambda value dropped |
| Horror Movies | [41:10](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2470s) | Gathering all variables except for title, so that the dataset is very tall |
| Horror Movies | [42:35](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2555s) | Using `unite` function to combine two variables (better alternative to `paste`) |
| Horror Movies | [45:45](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2745s) | Creating a new lasso with tons of new variables other than plot words |



***



#### NYC Squirrel Census

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| NYC Squirrel Census | [5:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=345s) | Starter EDA of latitude and longitude using `geom_point` |
| NYC Squirrel Census | [6:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=405s) | Aggregating squirrel counts by hectare to get a "binned" map |
| NYC Squirrel Census | [9:00](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=540s) | Investigating colour notes |
| NYC Squirrel Census | [10:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=630s) | Asking question, "Are there areas of the parks where we see certain-coloured squirrels |
| NYC Squirrel Census | [12:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=765s) | Plotting latitude and percentage of gray squirrels to answer, "Do we get a lower proportion of gray squirrels as we go farther north?" |
| NYC Squirrel Census | [13:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=810s) | Using logistic regression to test gray squirrel (proportion as we go farther north) |
| NYC Squirrel Census | [16:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=990s) | Noting that he could have used original data sets as input for logistic regression function |
| NYC Squirrel Census | [19:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1170s) | "Does a squirrel run away?" based on location in the park (latitude), using logistic regression |
| NYC Squirrel Census | [20:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1245s) | Using `summarise_at` function to apply same function to multiple variables |
| NYC Squirrel Census | [25:25](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1525s) | Loading `ggmap` package |
| NYC Squirrel Census | [27:00](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1620s) | Start using `ggmap`, with the `get_map` function |
| NYC Squirrel Census | [28:20](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1700s) | Decision to not set up Google API key to use `ggmap` properly |
| NYC Squirrel Census | [30:15](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1815s) | Using the `sf` package to read in a shapefile of Central Park |
| NYC Squirrel Census | [30:40](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1840s) | Using `read_sf` function from `sf` package to import a shapefile into R |
| NYC Squirrel Census | [31:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1890s) | Using `geom_sf` function from `sf` package to visualise the imported shapefile |
| NYC Squirrel Census | [32:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1965s) | Combining shapefile "background" with relevant squirrel data in one plot |
| NYC Squirrel Census | [34:40](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2080s) | Visualising pathways (footpaths, bicycle paths) in the shapefile |
| NYC Squirrel Census | [37:55](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2275s) | Finishing visualisation and moving on to analysing activity types |
| NYC Squirrel Census | [38:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2325s) | Selecting fields based on whether they end with "ing", then gathering those fields into tidy format |
| NYC Squirrel Census | [39:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2390s) | Decision to create a `shiny` visualisation |
| NYC Squirrel Census | [41:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2490s) | Setting `shiny` app settings (e.g., slider for minimum number of squirrels) |
| NYC Squirrel Census | [42:15](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2535s) | Setting up `shiny` app options / variables |
| NYC Squirrel Census | [43:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2630s) | Explanation of why setting up options in `shiny` app the way he did |
| NYC Squirrel Census | [46:00](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2760s) | Solving error "Discrete value supplied to continuous scale" |
| NYC Squirrel Census | [46:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2810s) | First draft of `shiny` app |
| NYC Squirrel Census | [48:35](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2915s) | Creating a dynamic midpoint for the two-gradient scale in the `shiny` app |
| NYC Squirrel Census | [51:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3090s) | Adding additional variables of more behaviours to `shiny` app (kuks, moans, runs from, etc.) |
| NYC Squirrel Census | [53:10](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3190s) | "What are the distributions of some of these behaviours?" |
| NYC Squirrel Census | [56:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3410s) | Adding ground location (above ground, ground plane) to `shiny` app |
| NYC Squirrel Census | [58:20](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3500s) | Summary of screencast |



***



#### CRAN Package Code

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| CRAN Package Code | [4:30](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=270s) | Summarizing many things by language (e.g., lines of code, comment/code ratio) |
| CRAN Package Code | [9:35](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=575s) | Using `gather` function (now `pivot_longer`) to consolidate multiple metrics into one dimension, then visualizing by facetting by metric |
| CRAN Package Code | [11:20](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=680s) | Setting ncol = 1 within `facet_wrap` function to get facetted graphs to stack vertically |
| CRAN Package Code | [11:30](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=690s) | Using `reorder_within` function from `tidytext` package to properly reorder factors within each facet |
| CRAN Package Code | [16:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=960s) | Using `geom_text` label to add language name as label to scatter points |
| CRAN Package Code | [20:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=1200s) | Completing preliminary overview and looking at distribution of R code in packages |
| CRAN Package Code | [26:15](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=1575s) | Using `str_extract` to extract only letters and names from character vector (using regex) |
| CRAN Package Code | [34:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=2040s) | Re-ordering the order of categorical variables in the legend using `guides` function |
| CRAN Package Code | [36:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=2160s) | Investigating comment/code ratio |
| CRAN Package Code | [43:05](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=2585s) | Importing additional package data (looking around for a bit, then starting to actually import ~46:00) |
| CRAN Package Code | [54:40](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3280s) | Importing even more additional data (available packages) |
| CRAN Package Code | [57:50](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3470s) | Using `separate_rows` function to separate delimited values |
| CRAN Package Code | [58:45](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3525s) | Using `extract` function and regex to pull out specific types of characters from a string |
| CRAN Package Code | [1:05:35](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3935s) | Summary of screencast |


